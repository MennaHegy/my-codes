{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3rYzYGoynaMP"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-50c1e1a564a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m  \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as  plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import mixture\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K0sux87lnnyF"
   },
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(28, 28),\n",
    "                  cmap='binary', interpolation='nearest',\n",
    "                  clim=(0, 56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "z8XBLr3eG8Ht",
    "outputId": "2301e51a-c37c-4b15-cc70-5f59a6c6ed9f"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(y_train[:10])\n",
    "\n",
    "print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))\n",
    "print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))\n",
    "\n",
    "plot_digits(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "KPLvRQs7jZNI",
    "outputId": "74ce0c80-88a0-4c1e-a404-b8faf1c427a6"
   },
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA().fit(x_train.reshape(60000,28*28))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "JCcMqrf6kVof",
    "outputId": "ea4ad153-161d-4063-8a92-4b1d304be9e5"
   },
   "outputs": [],
   "source": [
    "pca = PCA(0.5)  # project from 64 to 2 dimensions\n",
    "\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(size=x_train.shape,scale=50)\n",
    "noisy=x_train+noise\n",
    "plot_digits(noisy)\n",
    "\n",
    "components = pca.fit_transform(noisy.reshape(60000,28*28))\n",
    "filtered = pca.inverse_transform(components)\n",
    "plot_digits(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "uRiUkcHGNx-X",
    "outputId": "77420ffe-7116-4fee-f81a-43b77e0dfc13"
   },
   "outputs": [],
   "source": [
    "pca = PCA()  # project from 64 to 2 dimensions\n",
    "\n",
    "components = pca.fit_transform(noisy.reshape(60000,28*28))\n",
    "filtered = pca.inverse_transform(components)\n",
    "\n",
    "noise=pca.explained_variance_ratio_\n",
    "plt.plot(np.cumsum(noise))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('level of impurity or noise');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPidgY3Xkcw3"
   },
   "outputs": [],
   "source": [
    "def add_salt_pepper_noise(X_imgs,salt_vs_pepper=0.9,amount=0.1):\n",
    "    # Need to produce a copy as to not modify the original image\n",
    "    X_imgs_copy = X_imgs.copy()\n",
    "    row, col = X_imgs_copy[0].shape\n",
    "    num_salt = np.ceil(amount * X_imgs_copy[0].size * salt_vs_pepper)\n",
    "    num_pepper = np.ceil(amount * X_imgs_copy[0].size * (1.0 - salt_vs_pepper))\n",
    "    \n",
    "    for X_img in X_imgs_copy:\n",
    "        # Add Salt noise\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt)) for i in X_img.shape]\n",
    "        X_img[coords[0], coords[1]] = 255\n",
    "\n",
    "        # Add Pepper noise\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in X_img.shape]\n",
    "        X_img[coords[0], coords[1]] = 0\n",
    "    return X_imgs_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "Jw9XZzaUK1Nv",
    "outputId": "d2dd52f4-fc7e-4b45-8b0c-1469d86e0fff"
   },
   "outputs": [],
   "source": [
    "\n",
    "salt_pepper_noise_imgs = add_salt_pepper_noise(x_train)\n",
    "plot_digits(salt_pepper_noise_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "dVpf-31mOQtX",
    "outputId": "6ec58648-37b1-4a1c-fa44-308e05de5b2a"
   },
   "outputs": [],
   "source": [
    "pca = PCA(0.43)  # project from 64 to 2 dimensions\n",
    "\n",
    "\n",
    "components = pca.fit_transform(salt_pepper_noise_imgs.reshape(60000,28*28))\n",
    "filtered = pca.inverse_transform(components)\n",
    "plot_digits(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "0dlMJJmdPlvJ",
    "outputId": "a8028688-015d-46fe-bd38-5ee477db0729"
   },
   "outputs": [],
   "source": [
    "pca = PCA()  # project from 64 to 2 dimensions\n",
    "\n",
    "\n",
    "components = pca.fit_transform(salt_pepper_noise_imgs.reshape(60000,28*28))\n",
    "filtered = pca.inverse_transform(components)\n",
    "noise=pca.explained_variance_ratio_\n",
    "plt.plot(np.cumsum(noise))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('level of impurity or noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzIRHp8-yPte"
   },
   "outputs": [],
   "source": [
    "def Generate_data_with_labels_1(n=100,range=1000,noise=300):\n",
    "  x1 = np.random.randint(range,size=n).reshape(n,1)\n",
    "  \n",
    "  noise2=np.random.normal(5,noise,size=n).reshape(n,1)\n",
    "  noise3=np.random.normal(50,noise,size=n).reshape(n,1)\n",
    "  noise4=np.random.normal(100,noise,size=n).reshape(n,1)\n",
    "  \n",
    "  x2=x1*3 +noise2\n",
    "  x3=-x1*15 +noise3\n",
    "  x4=x1*1 +noise4\n",
    "  \n",
    "  x=np.concatenate((x1,x2,x3,x4),axis=1)\n",
    "  y=np.array(x2<=range/2).astype(int).reshape(-1,)\n",
    "  \n",
    "  X=pd.DataFrame(x)\n",
    "  Y=pd.DataFrame(y)\n",
    "  \n",
    "  x_train, x_test, y_train, y_test = train_test_split(X,Y, random_state=42)\n",
    "\n",
    "  return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "id": "Xffj3SqoYn7L",
    "outputId": "b03f854f-3755-4912-9fbd-966621d9942e"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = Generate_data_with_labels_1()\n",
    "\n",
    "\n",
    "\n",
    "sns.pairplot(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALP2vsjNzCs9"
   },
   "outputs": [],
   "source": [
    "def test_lda_apply_decorrelate_test_lda(x_train, y_train,x_test, y_test):\n",
    "  clf = LinearDiscriminantAnalysis()\n",
    "  clf.fit(x_train, y_train)\n",
    "  print('score : ',clf.score(x_test, y_test))\n",
    "  \n",
    "  pca = PCA()  \n",
    "  components = pca.fit_transform(x_train)\n",
    "  sns.scatterplot(components[:,0],components[:,1],hue=y_train.to_numpy().reshape(-1,))\n",
    "  plt.show()\n",
    "\n",
    "  clf = LinearDiscriminantAnalysis('svd')\n",
    "  clf.fit_transform(components, y_train)\n",
    "  print(clf.score(components, y_train))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "bbGdUfHB3VBG",
    "outputId": "48601203-03f5-46b8-e30e-e16d06e9beb4"
   },
   "outputs": [],
   "source": [
    "test_lda_apply_decorrelate_test_lda(x_train, y_train,x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M50GA5Cpnnhv"
   },
   "outputs": [],
   "source": [
    "def Generate_data_with_labels_2(num_samples = 400):\n",
    "  # The desired mean values of the sample.\n",
    "  mu = np.array([5.0, 0.0, 10.0])\n",
    "\n",
    "  # The desired covariance matrix.\n",
    "  r = np.array([\n",
    "        [  3.40, -2.75, -2.00],\n",
    "        [ -2.75,  5.50,  1.50],\n",
    "        [ -2.00,  1.50,  1.25]\n",
    "    ])\n",
    "  \n",
    "  \n",
    "  # Generate the random samples.\n",
    "  np.random.seed(42)\n",
    "  x = np.random.multivariate_normal(mu, r, size=num_samples)\n",
    "  # Generate the labels.\n",
    "  #line=(x[:,1]*0.3)+10\n",
    "  split=(x[:,2]<=9)+(x[:,2]>=11)\n",
    "  #split=(x[:,2]<=9)*1+(x[:,2]>=11)*2+((x[:,2]<11)&(x[:,2]>9))*3\n",
    "  y=np.array(split).astype(int).reshape(-1,)\n",
    "\n",
    "\n",
    "  X=pd.DataFrame(x)\n",
    "  Y=pd.DataFrame(y)\n",
    "  \n",
    "  x_train, x_test, y_train, y_test = train_test_split(X,Y, random_state=42)\n",
    "\n",
    " return x_train, x_test, y_train, y_test\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "ZF5hrk1Q0pT9",
    "outputId": "a1e38b5e-f1ec-4afe-89f0-ffc1495db44f"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = Generate_data_with_labels_2()\n",
    "sns.pairplot(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "GmS8QR9NrG4v",
    "outputId": "40e96f4c-c3d8-4108-f518-f1400fbaa7e7"
   },
   "outputs": [],
   "source": [
    "test_lda_apply_decorrelate_test_lda(x_train, y_train,x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvVuRzR4sMgU"
   },
   "outputs": [],
   "source": [
    "def Generate_data_with_labels_3(n_samples = 1500,random_state = 170):\n",
    "  x, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "  # Anisotropicly distributed data\n",
    "  transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "  X_aniso = np.dot(x, transformation)\n",
    "  gmm =mixture.GaussianMixture(n_components=3, covariance_type='full')\n",
    "  gmm.fit(X_aniso)\n",
    "  y_pred =gmm.predict(X_aniso)\n",
    "  \n",
    "  X=pd.DataFrame(X_aniso)\n",
    "  Y=pd.DataFrame(y_pred)\n",
    "  \n",
    "  x_train, x_test, y_train, y_test = train_test_split(X,Y, random_state=42)\n",
    "\n",
    "  return x_train, x_test, y_train, y_test\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "XawIAfIj8Cur",
    "outputId": "3b9e8172-8edf-4170-b653-c62888dc973f"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = Generate_data_with_labels_3()\n",
    "sns.scatterplot(x_train[0].to_numpy().reshape(-1,),x_train[1].to_numpy().reshape(-1,),hue=y_train.to_numpy().reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "Q_ALcEwU9nSq",
    "outputId": "d797d0bd-48b2-461b-dd6e-8b3304e3bb14"
   },
   "outputs": [],
   "source": [
    "test_lda_apply_decorrelate_test_lda(x_train, y_train,x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVwm3xXK-XZr"
   },
   "outputs": [],
   "source": [
    "def Generate_data_with_labels_4():\n",
    "  penguins = sns.load_dataset(\"penguins\")\n",
    "  penguins = penguins.replace([np.inf, -np.inf], np.nan)\n",
    "  penguins = penguins.fillna(penguins.mean())\n",
    "  x=penguins.drop(columns=['species','island','sex'])\n",
    "  y=((penguins['species']=='Adelie')*1)+((penguins['species']=='Chinstrap')*2)+((penguins['species']=='Gentoo')*3)\n",
    "\n",
    "  X=pd.DataFrame(x)\n",
    "  Y=pd.DataFrame(y)\n",
    "  \n",
    "  x_train, x_test, y_train, y_test = train_test_split(X,Y, random_state=42)\n",
    "\n",
    "  return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "id": "AqMrfwYI_Zgi",
    "outputId": "07d7ccc3-967b-483d-f326-3ffb90f7b4c1"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = Generate_data_with_labels_4()\n",
    "\n",
    "\n",
    "penguins = sns.load_dataset(\"penguins\")\n",
    "sns.pairplot(penguins,hue='species')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "ON5ZE3Oz-u--",
    "outputId": "2b4bbd84-657c-4bbb-dcc1-269a0db570cb"
   },
   "outputs": [],
   "source": [
    "test_lda_apply_decorrelate_test_lda(x_train, y_train,x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML2_Project_Matrix_Decomposition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
